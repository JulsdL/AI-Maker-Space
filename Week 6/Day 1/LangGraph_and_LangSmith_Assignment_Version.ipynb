{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "d2b0dd31-273b-4f63-b019-50e7eddd15b1"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "db73ce2a-b96e-475c-df1a-374ecf32fdef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "82647f16-a782-48eb-c0e5-568b8896cdc6"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(), ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### Answer #1: The model uses its own \"judgement\" to use a tool or not and to determine which tool to use in order to complete the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### Answer #2: There is no specific limit to how many times we can cycle. We could impose a limit to the number of cycles by adding a conditional check to the graph, by adding a new conditional edge that checks the number of cycles and stops the graph when the limit is reached, or enhancing the actual conditional edge \"should_continue\" function to check the number of cycles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "db0f32d3-724a-4f64-9a48-121ee2ddfb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"RAG in the context of Large Language Models\"}\n",
            "Tool Response: Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. February 2024 marked a pivotal moment in the AI field as Google unveiled Gemini 1.5 Pro, boasting an impressive 1M tokens of context capacity.This advance, closely following OpenAI's \"turbo\" enhancement to GPT-4 and Anthropic's Claude 2.1, has reignited the discourse on the relevance of Retrieval-Augmented Generation (RAG) in an era dominated by Large Context Models. The augmentation component of an RAG system refines the context for the language model. This component uses the retrieved information to improve the prompt, which then guides better response generation. ... (RAG) and Large Language Models (LLMs) technologies can be leveraged for customer support automation, providing a more efficient and ... The year 2024 seems to be one of long contexts, quite literally. There's Anthropic's Claude 3 with a context window of 200K (goes up to 1 million tokens for specific use cases) and Google's Gemini 1.5 with a 1M context length window.. Meta's Llama 3 has become the latest muse for the online developer community with many users coming up with wild use cases everyday such as that by ... Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren't included in the models' original ...\n",
            "\n",
            "Agent Response: In the context of Large Language Models, RAG stands for Retrieval-Augmented Generation. RAG is a technique used to \"ground\" large language models with specific data sources, often sources that weren't included in the models' original training data. RAG helps LLMs understand the broader context of information by retrieving relevant information to improve response generation.\n",
            "\n",
            "RAG broke onto the scene in February 2024 when Google unveiled Gemini 1.5 Pro, which boasted an impressive 1 million tokens of context capacity. This advancement in the AI field reignited the discourse on the relevance of RAG in an era dominated by Large Context Models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "88063264-fdef-4bb3-a7c5-3a3eb1a1cff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon. ... LoRA, Machine Learning, QLoRA, transformers. Categories: data-science, machine-learning. Updated: July 26, 2023. Share on Twitter Facebook ... QLoRA is a fine-tuning method that combines Quantization and Low-Rank Adapters (LoRA). QLoRA is revolutionary in that it democratizes fine-tuning: it enables one to fine-tune massive models with billions of parameters on relatively small, highly available GPUs. ... QLoRA stands as a significant advancement in the field of machine learning ...\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: QLoRA (Quantized Low-Rank Adapters) is an efficient fine-tuning approach in machine learning that reduces memory usage significantly, allowing for the fine-tuning of large language models on relatively small GPUs while preserving performance. QLoRA combines quantization and low-rank adapters to achieve this memory efficiency.\n",
            "\n",
            "One of the technical papers related to QLoRA is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper presents QLoRA as an approach that enables the fine-tuning of a 65B parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning task performance. The authors introduce innovations such as 4-bit NormalFloat (NF4) data type, double quantization, and paged optimizers to save memory without sacrificing performance.\n",
            "\n",
            "Tim Dettmers is one of the authors of the QLoRA paper. His research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "#### 1.The agent receives the state object with the user's request, decides it needs a tool and passes the state along the conditional edge.\n",
        "#### 2. As a tool is called, the conditional edge output is to continue \"action\", so the state object is passed to the action node where the agent calls the first tool: DuckDuckGo.\n",
        "#### 3. The action node adds the response from the DuckDuckGo tool to the state object and passes it along the edge to the agent node.\n",
        "#### 4. The agent node decides it needs a second tool, adds its response to the state object and passes it along the conditional edge again.\n",
        "#### 5. As a tool is called again, the conditional edge output is to continue, so the state object is passed to the action node where the second tool: Arxiv is called.\n",
        "#### 6. The action node adds the response from the Arxiv tool to the state object and passes it along the edge to the agent node.\n",
        "#### 7. The agent node adds create it's final answer and doesn't call a tool, it adds its response to the state object and passes it along the conditional edge again.\n",
        "#### 8. The conditional edge output is to end, so the state object is passed to the END node where the final answer is output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An eclipsing binary is a type of binary star system in which the two stars orbit each other in such a way that they periodically eclipse each other as seen from Earth. This results in a regular pattern of brightness variations, known as light curves, as the stars pass in front of each other. Eclipsing binaries are important in astronomy because they allow astronomers to determine the physical properties of the stars, such as their sizes, masses, and temperatures, by analyzing the light curves. This information can provide valuable insights into stellar evolution and the behavior of binary star systems.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is an Eclipsing Binary?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is an eclipsing binary star?\",\n",
        "    \"How are the components of eclipsing binary systems commonly categorized?\",\n",
        "    \"What is the primary method used to study eclipsing binary stars?\",\n",
        "    \"Why are eclipsing binaries important for astronomical measurements?\",\n",
        "    \"What can the light curves of eclipsing binaries tell us?\",\n",
        "    \"How does the inclination of the orbital plane affect the observation of eclipsing binaries?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"binary\", \"system\", \"star\", \"orbit\", \"eclipse\"]},\n",
        "    {\"must_mention\" : [\"primary\", \"secondary\", \"star\", \"types\"]},\n",
        "    {\"must_mention\" : [\"photometry\", \"light\", \"curves\"]},\n",
        "    {\"must_mention\" : [\"distance\", \"mass\", \"diameter\", \"luminosity\"]},\n",
        "    {\"must_mention\" : [\"period\", \"brightness\", \"changes\"]},\n",
        "    {\"must_mention\" : [\"inclination\", \"orbital\", \"visible\", \"eclipse\"]},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    # description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        "    description=\"Questions about Eclipsing Binary Stars to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "#### Answer #3: The mapping is done in the create_examples function, where each question from the questions list is transformed into an input dictionary and placed in the inputs sequence. Then each corresponding set of terms from the answers list is transformed into an output dictionary and placed in the outputs sequence. The function ensures that for each question in the inputs list, there is a corresponding output in the outputs list. This correspondence is maintained by the order of the elements in these lists: the first question is paired with the first answer, the second question with the second answer, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "#### Answer # 4: The exact-match approach does not account for synonyms or paraphrased versions of the required terms. This could lead to lower scores even when the response effectively addresses the question with equivalent but differently phrased terms. Instead of a \"must_mention\" we could use a \"same_meaning\" approach. This would allow for a more flexible evaluation that considers the meaning of the terms used in the response rather than the exact terms themselves. We could use the string_distance evaluator as well. At last, we could make sure to ignore case sensitivity and punctuation in the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "045f1296-2bee-43c8-8b92-b24dc242ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - b1419ab9' at:\n",
            "https://smith.langchain.com/o/34de6ad9-fdc1-5905-a56b-32ac703aa2c6/datasets/5daa7de2-0648-41e2-b16c-218701161539/compare?selectedSessions=dc4cab59-2967-4086-b2ac-f9df14a69b8a\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 3414dc22 at:\n",
            "https://smith.langchain.com/o/34de6ad9-fdc1-5905-a56b-32ac703aa2c6/datasets/5daa7de2-0648-41e2-b16c-218701161539\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a4f64bf6-b161-498b-92c1-4e932c285e8f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.476238</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.819157</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.369187</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.105437</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.324096</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.726262</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.947180</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                               6.0   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                               1.0   \n",
              "std                      0.0                               0.0   \n",
              "min                      1.0                               1.0   \n",
              "25%                      1.0                               1.0   \n",
              "50%                      1.0                               1.0   \n",
              "75%                      1.0                               1.0   \n",
              "max                      1.0                               1.0   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                    False   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        5.476238   \n",
              "std                      NaN   NaN        2.819157   \n",
              "min                      NaN   NaN        2.369187   \n",
              "25%                      NaN   NaN        3.105437   \n",
              "50%                      NaN   NaN        5.324096   \n",
              "75%                      NaN   NaN        7.726262   \n",
              "max                      NaN   NaN        8.947180   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     a4f64bf6-b161-498b-92c1-4e932c285e8f  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - b1419ab9',\n",
              " 'results': {'fa97c0c3-c732-4e66-9616-a7b8cd7f2ceb': {'input': {'question': 'What is an eclipsing binary star?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of what an eclipsing binary star is. It explains how the two stars in this system orbit each other and how this results in periodic eclipses as seen from Earth. It also explains the resulting brightness variations and why these stars are important in astronomy. \\n\\nThe submission is helpful because it provides a comprehensive answer to the question. It is insightful because it explains not just what an eclipsing binary star is, but also why it is important in astronomy. The submission is appropriate because it directly answers the question and does not include any irrelevant information. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4b8b73f-6b51-4034-ab7c-4be4c42aab60'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies an eclipsing binary star as a binary star system where the two stars orbit each other in a way that they periodically eclipse each other. The student also correctly explains the significance of eclipsing binary stars in astronomy, which is not directly asked in the question but provides additional correct information. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d0cbe037-c1bf-471d-a928-e0ad4c67bd8d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8e064fa4-b049-4ac3-ab85-47b5d4742f0f'), target_run_id=None)],\n",
              "   'execution_time': 2.369187,\n",
              "   'run_id': 'a4f64bf6-b161-498b-92c1-4e932c285e8f',\n",
              "   'output': 'An eclipsing binary star is a binary star system in which the two stars orbit each other in such a way that they periodically eclipse each other as seen from Earth. This results in a regular pattern of brightness variations, with the total brightness of the system changing as one star passes in front of the other. Eclipsing binary stars are important in astronomy because they allow astronomers to determine the physical properties of the stars, such as their sizes, masses, and temperatures, by studying the light curves during the eclipses.',\n",
              "   'reference': {'must_mention': ['binary',\n",
              "     'system',\n",
              "     'star',\n",
              "     'orbit',\n",
              "     'eclipse']}},\n",
              "  'bd7b4f25-0c3d-4e1f-ba0f-3d495fd879fa': {'input': {'question': 'How are the components of eclipsing binary systems commonly categorized?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how the components of eclipsing binary systems are commonly categorized. It breaks down the categorization into three main categories, explaining each one in a clear and understandable manner. \\n\\nThe submission is helpful as it provides a comprehensive answer to the question, explaining the categorization based on relative sizes, masses, and luminosities of the stars in the binary system. It also explains the categorization based on the shape of the light curve during eclipses. \\n\\nThe submission is insightful as it not only provides the categories but also explains why these categories are important for astronomers in classifying and studying eclipsing binary systems. \\n\\nThe submission is appropriate as it directly answers the question and provides relevant information. \\n\\nBased on the above reasoning, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fbab4833-239c-4a27-a55b-38c8978d0550'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and aligns with the context provided. The student correctly identifies the primary and secondary stars as the two main components of an eclipsing binary system, and explains how they are categorized based on their relative sizes, masses, and luminosities. The student also correctly mentions that eclipsing binary systems can be further categorized based on the shape of their light curves during eclipses. The student's answer does not contradict any information in the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('34722294-4b94-44ba-a33d-eb6a1168445b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6791fa14-2af9-49a8-ac44-33269c72388f'), target_run_id=None)],\n",
              "   'execution_time': 5.038914,\n",
              "   'run_id': 'f6605f1e-0d76-46dd-837c-74d7229df2ae',\n",
              "   'output': 'The components of eclipsing binary systems are commonly categorized based on their relative sizes, masses, and luminosities. There are three main categories:\\n\\n1. Primary Star (Component A): The larger and more massive star in the binary system is known as the primary star. It is usually more luminous than the secondary star and is often referred to as the \"brighter\" star in the system.\\n\\n2. Secondary Star (Component B): The smaller and less massive star in the binary system is known as the secondary star. It is usually less luminous than the primary star and is often referred to as the \"fainter\" star in the system.\\n\\n3. Classification based on the shape of the light curve: Eclipsing binary systems are also categorized based on the shape of their light curves during eclipses. This includes categories such as detached binaries, semi-detached binaries, and contact binaries, depending on how close the stars are to each other and whether they share material.\\n\\nThese categories help astronomers classify and study eclipsing binary systems based on their physical properties and observational characteristics.',\n",
              "   'reference': {'must_mention': ['primary', 'secondary', 'star', 'types']}},\n",
              "  '651b3e72-5cd0-41f0-972f-7e1eb900223c': {'input': {'question': 'What is the primary method used to study eclipsing binary stars?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of the primary method used to study eclipsing binary stars, which is the photometric method. It explains how this method works, what it involves, and what kind of information it provides. \\n\\nThe submission is insightful as it not only names the method but also explains how it is used to determine the properties of the stars and the nature of the binary system. \\n\\nThe submission is appropriate as it directly answers the question asked in the input and provides relevant information. \\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f963aac4-d531-4c51-a35f-912d235c0266'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies photometry as the primary method used to study eclipsing binary stars. The student also accurately describes the process of using photometry to study these stars, including the use of light curves and the type of information that can be gleaned from this method. The context provided ('photometry', 'light', 'curves') aligns with the student's answer. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('aad8cb09-437a-413f-8a20-6cf88c7f3f86'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('276fdc14-64a6-4e47-b04d-55d82831edaa'), target_run_id=None)],\n",
              "   'execution_time': 2.460944,\n",
              "   'run_id': '9fcdfd90-fa89-4349-98ed-98f172231c43',\n",
              "   'output': 'The primary method used to study eclipsing binary stars is the photometric method. This method involves measuring the changes in brightness of the binary star system as one star passes in front of the other, causing an eclipse. By analyzing the light curve of the system, astronomers can determine the properties of the stars, such as their sizes, masses, temperatures, and orbital parameters. This method provides valuable information about the nature of the binary system and helps astronomers understand the dynamics of these systems.',\n",
              "   'reference': {'must_mention': ['photometry', 'light', 'curves']}},\n",
              "  '8b728224-9250-4bcf-937b-c39ff83d020a': {'input': {'question': 'Why are eclipsing binaries important for astronomical measurements?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of why eclipsing binaries are important for astronomical measurements. It lists five reasons, each with a clear explanation. This makes the submission helpful as it provides a comprehensive answer to the question.\\n\\nThe submission is also insightful. It not only lists the reasons but also explains how eclipsing binaries contribute to each of these areas. This gives the reader a deeper understanding of the topic.\\n\\nLastly, the submission is appropriate. It directly answers the question and stays on topic throughout. The language used is suitable for the subject matter and the submission is well-structured, making it easy to follow.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3731b54c-1a3a-4499-884d-29af03c960cb'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and covers all the points mentioned in the context. The student explains how eclipsing binaries help in determining stellar parameters, testing stellar models, measuring distances, studying stellar variability, and detecting exoplanets. All these points are factually correct and relevant to the question. The student's answer does not contradict any information given in the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('30ba76ec-6f09-454b-b5a5-f12f32094043'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('889c5e60-bfd4-462d-a3ea-5e43b2679905'), target_run_id=None)],\n",
              "   'execution_time': 8.94718,\n",
              "   'run_id': 'f8b0dc65-2102-447a-8d40-5365b7ba0716',\n",
              "   'output': 'Eclipsing binaries are important for astronomical measurements because they provide valuable information about the properties of stars. Here are some reasons why eclipsing binaries are important:\\n\\n1. **Determination of stellar parameters**: Eclipsing binaries consist of two stars that orbit each other in such a way that they periodically eclipse each other as seen from Earth. By studying the light curve of these eclipses, astronomers can determine the sizes, masses, temperatures, and luminosities of the stars in the binary system. This information helps in understanding the fundamental properties of stars.\\n\\n2. **Testing stellar models**: Eclipsing binaries provide a unique opportunity to test theoretical models of stellar structure and evolution. By comparing the observed properties of the stars in eclipsing binaries with the predictions of stellar models, astronomers can improve our understanding of stellar evolution.\\n\\n3. **Distance measurements**: Eclipsing binaries can be used as standard candles to determine distances to star clusters and galaxies. By measuring the brightness of the stars in the binary system and comparing it to their known intrinsic luminosities, astronomers can calculate the distance to the system and, by extension, to the surrounding objects.\\n\\n4. **Stellar variability**: Eclipsing binaries exhibit variations in their light curves due to factors such as stellar activity, pulsations, and other phenomena. Studying these variations can provide insights into the internal structure and dynamics of stars.\\n\\n5. **Exoplanet detection**: Eclipsing binaries can also be used to detect exoplanets. When a planet passes in front of one of the stars in the binary system, it causes additional dimming in the light curve. By studying these transit events, astronomers can identify and characterize exoplanets orbiting the stars.\\n\\nOverall, eclipsing binaries play a crucial role in advancing our understanding of stellar astrophysics, testing theoretical models, and making important astronomical measurements.',\n",
              "   'reference': {'must_mention': ['distance',\n",
              "     'mass',\n",
              "     'diameter',\n",
              "     'luminosity']}},\n",
              "  '9d85a197-389f-493e-8eae-78be5c64eec8': {'input': {'question': 'What can the light curves of eclipsing binaries tell us?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of what the light curves of eclipsing binaries can tell us. It lists six key insights that can be obtained from studying the light curves of eclipsing binaries, including binary system parameters, stellar properties, eclipses, temperature variations, period changes, and stellar evolution. Each point is explained in a clear and understandable manner.\\n\\nThe submission is insightful as it not only lists the information that can be obtained from the light curves of eclipsing binaries, but also explains why this information is important and how it can help astronomers understand the properties and dynamics of binary star systems.\\n\\nThe submission is appropriate as it directly answers the input question and provides relevant and accurate information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2e961e77-5e24-4cf1-b896-f1ae0bdee634'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and covers all the aspects mentioned in the context. The student mentions that the light curves of eclipsing binaries can tell us about the period, which is mentioned in the context. The student also talks about changes in the orbital period and brightness, which are also mentioned in the context. The student's answer does not contradict the context in any way. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c92c39b7-09a1-4c8f-9496-564691b334e3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a2f96fde-d73d-423d-9b1a-3a3a10c2146f'), target_run_id=None)],\n",
              "   'execution_time': 8.431924,\n",
              "   'run_id': '2df001a3-3a31-4a67-aa96-60f73caabf21',\n",
              "   'output': 'The light curves of eclipsing binaries can tell us a lot of information about the binary star system. Here are some key insights that can be obtained from studying the light curves of eclipsing binaries:\\n\\n1. **Binary System Parameters**: By analyzing the light curve, astronomers can determine the orbital period of the binary system, the sizes and shapes of the stars, the distance between the stars, and the inclination of the orbital plane.\\n\\n2. **Stellar Properties**: The light curve can provide information about the individual stars in the binary system, such as their masses, radii, temperatures, and luminosities. This can help astronomers understand the evolutionary stage of the stars.\\n\\n3. **Eclipses**: Eclipsing binaries exhibit periodic dips in brightness when one star passes in front of the other (primary eclipse) or when both stars pass in front of each other (secondary eclipse). The depth and duration of these eclipses can reveal the relative sizes and temperatures of the stars.\\n\\n4. **Temperature Variations**: The light curve can show variations in brightness due to temperature differences between the stars. This can provide insights into the surface temperatures and atmospheres of the stars.\\n\\n5. **Period Changes**: Studying the light curve over time can reveal changes in the orbital period of the binary system, which can be caused by factors such as mass transfer between the stars or gravitational interactions with a third body.\\n\\n6. **Stellar Evolution**: Eclipsing binaries are valuable for studying stellar evolution, as the changes in brightness and other parameters over time can help astronomers understand how stars evolve and interact in binary systems.\\n\\nOverall, the light curves of eclipsing binaries provide a wealth of information about the properties and dynamics of binary star systems, making them important objects of study in astronomy.',\n",
              "   'reference': {'must_mention': ['period', 'brightness', 'changes']}},\n",
              "  'b96b60d8-034f-4474-9c43-4738aa73797f': {'input': {'question': 'How does the inclination of the orbital plane affect the observation of eclipsing binaries?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how the inclination of the orbital plane affects the observation of eclipsing binaries. It explains the concept of an eclipsing binary system and how the inclination angle of the orbital plane determines the depth and duration of the eclipses we observe. \\n\\nThe submission also explains how studying the light curves of eclipsing binaries at different inclination angles can help astronomers determine the physical properties of the stars. This information is helpful for understanding the dynamics and evolution of binary star systems.\\n\\nBased on this analysis, the submission is helpful, insightful, and appropriate. It provides a comprehensive answer to the question and offers additional information that enhances understanding of the topic.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5c58aa44-953f-48f7-a43d-c4fe87680892'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The student correctly explains how the inclination of the orbital plane affects the observation of eclipsing binaries. They mention that the inclination angle determines the depth and duration of the eclipses observed. They also correctly state that if the orbital plane is nearly edge-on to our line of sight, we will see the stars fully eclipse each other, and if the orbital plane is nearly face-on to our line of sight, we may only see a partial eclipse or no eclipse at all. The student also correctly explains that by studying the light curves of eclipsing binaries at different inclination angles, astronomers can determine the physical properties of the stars. \\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a9962100-21fa-4b4d-9f6b-6a778492f09b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('810bd0e1-3b76-445a-a0ae-e1f88bce6949'), target_run_id=None)],\n",
              "   'execution_time': 5.609277,\n",
              "   'run_id': '6d63ff87-4d9b-4af8-a744-3c114745d6ee',\n",
              "   'output': 'The inclination of the orbital plane of eclipsing binaries affects the way we observe the eclipses. \\n\\nIn an eclipsing binary system, two stars orbit around each other in a plane. When the orbital plane is inclined in such a way that it is aligned with our line of sight from Earth, we observe the stars passing in front of each other, causing eclipses. \\n\\nThe inclination angle of the orbital plane determines the depth and duration of the eclipses we observe. If the orbital plane is nearly edge-on to our line of sight, we will see the stars fully eclipse each other, resulting in a deep and long eclipse. On the other hand, if the orbital plane is nearly face-on to our line of sight, we may only see a partial eclipse or no eclipse at all.\\n\\nBy studying the light curves of eclipsing binaries at different inclination angles, astronomers can determine the physical properties of the stars, such as their sizes, masses, and temperatures. This information is crucial for understanding the dynamics and evolution of binary star systems.',\n",
              "   'reference': {'must_mention': ['inclination',\n",
              "     'orbital',\n",
              "     'visible',\n",
              "     'eclipse']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "#### That function evaluate the helpfulness of responses in a conversation. It first extracts the initial query and the final response from the conversation state. The function then uses a prompt template combined with a model to determine if the final response is extremely helpful (denoted by \"Y\") or not (denoted by \"N\"), and it provides feedback based on this assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWOPweuSiHc"
      },
      "source": [
        "Let's compile and test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Let's render out Graph in png !!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_core.runnables.graph import CurveStyle, NodeColors, MermaidDrawMethod\n",
        "from IPython.display import display, HTML, Image\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        agent_with_helpfulness_check.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "7e2ea696-ba42-468e-a842-2c585f246c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: Related to astronomy, what is an eclipsing binary star? Also, what are the main types of eclipsing binaries? Also, how does the orbital plane's inclination affect the observation of eclipses?\n",
            "\n",
            "\n",
            "Agent Response: An eclipsing binary star is a binary star system in which the two stars orbit around a common center of mass in such a way that they periodically eclipse each other as seen from Earth. This results in a periodic decrease in the system's overall brightness when one star passes in front of the other.\n",
            "\n",
            "There are several main types of eclipsing binaries based on the characteristics of the stars and their orbits:\n",
            "\n",
            "1. **Detached Eclipsing Binaries**: In this type, the two stars are well-separated and do not interact with each other. Both stars have their own Roche lobes and do not transfer mass between them.\n",
            "\n",
            "2. **Semi-Detached Eclipsing Binaries**: In this type, one star fills its Roche lobe while the other does not. The star that fills its Roche lobe is called the donor star, and it transfers mass to the other star, known as the accretor.\n",
            "\n",
            "3. **Contact Eclipsing Binaries**: In this type, both stars have filled their Roche lobes and are in contact with each other. They share a common envelope of gas and are in close proximity.\n",
            "\n",
            "The orbital plane's inclination affects the observation of eclipses in the following ways:\n",
            "\n",
            "- If the orbital plane is inclined such that it is nearly edge-on to the observer, the eclipses will be total and the stars will completely block each other's light, resulting in a significant decrease in brightness.\n",
            "- If the orbital plane is inclined such that it is nearly face-on to the observer, the eclipses will be partial and the stars will only partially block each other's light, resulting in a smaller decrease in brightness.\n",
            "- The inclination of the orbital plane also affects the duration and frequency of eclipses observed from Earth.\n",
            "\n",
            "Overall, the study of eclipsing binary stars provides valuable information about the properties of the stars, such as their sizes, masses, temperatures, and luminosities, as well as insights into stellar evolution and binary star dynamics.\n"
          ]
        }
      ],
      "source": [
        "# inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to astronomy, what is an eclipsing binary star? Also, what are the main types of eclipsing binaries? Also, how does the orbital plane's inclination affect the observation of eclipses?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
