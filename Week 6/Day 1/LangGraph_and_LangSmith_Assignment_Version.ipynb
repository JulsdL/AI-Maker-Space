{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "d2b0dd31-273b-4f63-b019-50e7eddd15b1"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "db73ce2a-b96e-475c-df1a-374ecf32fdef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "82647f16-a782-48eb-c0e5-568b8896cdc6"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(), ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### Answer #1: The model uses the tool description to determine which tool to use in order to complete the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "#### Answer #2: There is no specific limit to how many times we can cycle. We can impose a limit to the number of cycles by adding a conditional check to the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "db0f32d3-724a-4f64-9a48-121ee2ddfb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"RAG in the context of Large Language Models\"}\n",
            "Tool Response: Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. With the help of Large Language Models (LLMs), finally, we have a system that can understand(!) our questions and answer them. But, we know‚Ä¶ 9 min read ¬∑ Dec 5, 2023 RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process. Explore what you can do with IBM watsonx to deploy and embed AI across your business. Large language models can be inconsistent. Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren't included in the models' original ... RAG (Retrieval-Augmented Generation) is a pivotal technique in LLMs (Large Language Models), designed to mitigate errors or 'hallucinations.' It achieves this by basing model responses on ...\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation in the context of Large Language Models (LLMs). It is an AI framework for retrieving facts from an external knowledge base to ground large language models on accurate and up-to-date information. RAG helps provide insight into the generative process of LLMs and improves their performance by incorporating external knowledge. RAG broke onto the scene as a pivotal technique in LLMs to mitigate errors or 'hallucinations' by basing model responses on specific data sources that were not included in the original models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "88063264-fdef-4bb3-a7c5-3a3eb1a1cff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon. ... LoRA, Machine Learning, QLoRA, transformers. Categories: data-science, machine-learning. Updated: July 26, 2023. Share on Twitter Facebook ... QLoRA is a fine-tuning method that combines Quantization and Low-Rank Adapters (LoRA). QLoRA is revolutionary in that it democratizes fine-tuning: it enables one to fine-tune massive models with billions of parameters on relatively small, highly available GPUs. ... QLoRA stands as a significant advancement in the field of machine learning ...\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: QLoRA (Quantized Low-Rank Adapters) is an efficient fine-tuning approach in machine learning that reduces memory usage significantly, allowing for the fine-tuning of large language models on relatively small GPUs while preserving performance. QLoRA combines quantization and low-rank adapters to achieve this memory efficiency.\n",
            "\n",
            "One of the technical papers related to QLoRA is titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. The paper presents QLoRA as an approach that enables the fine-tuning of a 65B parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning task performance. The authors introduce innovations such as 4-bit NormalFloat (NF4) data type, double quantization, and paged optimizers to save memory without sacrificing performance.\n",
            "\n",
            "Tim Dettmers is one of the authors of the QLoRA paper. His research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "#### 1.The agent receives the state object with the user's request and passes it along the conditional edge and decides to continue.\n",
        "#### 2. The state object is passed to the action node where the agent calls the first tool: DuckDuckGo.\n",
        "#### 3. The action node adds the response from the DuckDuckGo tool to the state object and passes it along the edge to the agent node.\n",
        "#### 4. The agent node adds a response to the state object and passes it along the conditional edge again, and decides to continue.\n",
        "#### 5. The agent passes the state object to the action node which calling the second tool: Arxiv.\n",
        "#### 6. The action node adds the response from the Arxiv tool to the state object and passes it along the edge to the agent node.\n",
        "#### 7. The agent node adds a response to the state object and passes it along the conditional edge again, and decides to end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An eclipsing binary is a type of binary star system in which the two stars orbit each other in such a way that they periodically eclipse each other as seen from Earth. This results in a regular pattern of brightness variations, with the total brightness of the system changing as one star passes in front of the other.\\n\\nEclipsing binaries are important in astronomy because they allow astronomers to determine the physical properties of the stars, such as their sizes, masses, and temperatures, by analyzing the light curves during the eclipses. This information can provide valuable insights into the nature of the stars and their evolution.\\n\\nThere are different types of eclipsing binaries, including detached binaries, semi-detached binaries, and contact binaries, depending on the proximity of the stars and the degree of interaction between them. Eclipsing binaries are commonly studied to understand stellar evolution, binary star formation, and other astrophysical phenomena.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is an Eclipsing Binary?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is an eclipsing binary star?\",\n",
        "    \"How are the components of eclipsing binary systems commonly categorized?\",\n",
        "    \"What is the primary method used to study eclipsing binary stars?\",\n",
        "    \"Why are eclipsing binaries important for astronomical measurements?\",\n",
        "    \"What can the light curves of eclipsing binaries tell us?\",\n",
        "    \"How does the inclination of the orbital plane affect the observation of eclipsing binaries?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"binary\", \"system\", \"star\", \"orbit\", \"eclipse\"]},\n",
        "    {\"must_mention\" : [\"primary\", \"secondary\", \"star\", \"types\"]},\n",
        "    {\"must_mention\" : [\"photometry\", \"light\", \"curves\"]},\n",
        "    {\"must_mention\" : [\"distance\", \"mass\", \"diameter\", \"luminosity\"]},\n",
        "    {\"must_mention\" : [\"period\", \"brightness\", \"changes\"]},\n",
        "    {\"must_mention\" : [\"inclination\", \"orbital\", \"visible\", \"eclipse\"]},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    # description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        "    description=\"Questions about Eclipsing Binary Stars to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "045f1296-2bee-43c8-8b92-b24dc242ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - e9eb8be3' at:\n",
            "https://smith.langchain.com/o/34de6ad9-fdc1-5905-a56b-32ac703aa2c6/datasets/ac607e73-571f-48fa-b230-eca93154b3b7/compare?selectedSessions=3365b097-5bb1-4694-97c4-b632e3029db0\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - ae331d85 at:\n",
            "https://smith.langchain.com/o/34de6ad9-fdc1-5905-a56b-32ac703aa2c6/datasets/ac607e73-571f-48fa-b230-eca93154b3b7\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>e492a972-25b1-4a56-abea-f31f10f29de6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.243929</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.627683</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.018882</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.096025</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.563724</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.127423</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.395127</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                               6.0   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                               1.0   \n",
              "std                      0.0                               0.0   \n",
              "min                      1.0                               1.0   \n",
              "25%                      1.0                               1.0   \n",
              "50%                      1.0                               1.0   \n",
              "75%                      1.0                               1.0   \n",
              "max                      1.0                               1.0   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                    False   NaN             NaN   \n",
              "freq                       5   NaN             NaN   \n",
              "mean                     NaN   NaN        4.243929   \n",
              "std                      NaN   NaN        1.627683   \n",
              "min                      NaN   NaN        2.018882   \n",
              "25%                      NaN   NaN        3.096025   \n",
              "50%                      NaN   NaN        4.563724   \n",
              "75%                      NaN   NaN        5.127423   \n",
              "max                      NaN   NaN        6.395127   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     e492a972-25b1-4a56-abea-f31f10f29de6  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - e9eb8be3',\n",
              " 'results': {'9a588aa0-212a-44ff-93c4-186f28b887d6': {'input': {'question': 'What is an eclipsing binary star?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of what an eclipsing binary star is, including how it works and why it is important in the field of astronomy. It explains the concept in a way that is easy to understand, and it provides additional information that could be useful to someone trying to learn about this topic. \\n\\nThe submission is insightful because it goes beyond just defining an eclipsing binary star. It also explains how studying these stars can provide valuable information about their properties and contribute to our understanding of stellar evolution and the dynamics of binary star systems.\\n\\nThe submission is appropriate because it directly answers the question and stays on topic. It does not include any irrelevant or inappropriate information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2196b5d5-e6ab-4248-8fc0-7fdecd48ea38'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The student correctly identifies an eclipsing binary star as a binary star system where the two stars orbit each other in such a way that they periodically eclipse or transit in front of each other. The student also correctly explains the significance of eclipsing binary stars in astronomy, which is not directly asked in the question but provides additional accurate information. Therefore, the student's answer is factually correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0d705318-08a6-4a11-9797-4fe65e20d516'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('98c4e80d-9666-47c8-acc2-8b3fe51b08a1'), target_run_id=None)],\n",
              "   'execution_time': 2.753261,\n",
              "   'run_id': 'e492a972-25b1-4a56-abea-f31f10f29de6',\n",
              "   'output': 'An eclipsing binary star is a binary star system in which the two stars orbit each other in such a way that they periodically eclipse or transit in front of each other as seen from Earth. This results in a regular pattern of brightness variations, known as light curves, as the total brightness of the system changes during the eclipses.\\n\\nEclipsing binary stars are important in astronomy because they provide valuable information about the properties of the stars, such as their sizes, masses, temperatures, and distances. By studying the light curves of eclipsing binary stars, astronomers can determine the physical characteristics of the stars and gain insights into stellar evolution and the dynamics of binary star systems.',\n",
              "   'reference': {'must_mention': ['binary',\n",
              "     'system',\n",
              "     'star',\n",
              "     'orbit',\n",
              "     'eclipse']}},\n",
              "  '260fab36-9da5-4862-b103-25a4c2f4f64b': {'input': {'question': 'How are the components of eclipsing binary systems commonly categorized?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of how the components of eclipsing binary systems are commonly categorized. It lists the common categories and provides a brief description of each, explaining the characteristics that define them. This information is helpful for someone looking to understand the categorization of eclipsing binary systems.\\n\\nThe submission is also insightful. It not only lists the categories but also explains why these categories are important for astronomers. This gives the reader a deeper understanding of the subject.\\n\\nLastly, the submission is appropriate. It directly answers the question asked in the input and stays on topic throughout. It uses clear and understandable language, making it suitable for a wide range of readers.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c5cc0a1a-e99e-4f23-a6c8-66904bc23266'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and detailed. They correctly identify the primary and secondary stars as the main components of an eclipsing binary system, and they accurately describe the characteristics of these components. They also correctly categorize eclipsing binary systems into contact, detached, and semi-detached binaries, providing accurate descriptions for each. The student's answer aligns with the context provided, which includes 'primary', 'secondary', 'star', and 'types'. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('29406ae2-7109-469c-8053-d28674467f06'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('fd599909-78d9-476e-8825-42cf620cb67c'), target_run_id=None)],\n",
              "   'execution_time': 5.003132,\n",
              "   'run_id': 'e3840733-53a3-46d2-b297-7863f9f01d10',\n",
              "   'output': 'The components of eclipsing binary systems are commonly categorized based on their relative sizes, temperatures, and luminosities. Here are the common categories:\\n\\n1. Primary Star (Component A): The primary star in an eclipsing binary system is usually the larger and more massive star. It is often hotter and more luminous than the secondary star.\\n\\n2. Secondary Star (Component B): The secondary star in an eclipsing binary system is typically the smaller and less massive star. It is usually cooler and less luminous than the primary star.\\n\\n3. Contact Binary: In a contact binary system, the two stars are so close to each other that they share a common envelope of gas. The stars may be in contact with each other or very close together.\\n\\n4. Detached Binary: In a detached binary system, the two stars are well-separated and do not share a common envelope of gas. They orbit each other without significant interaction.\\n\\n5. Semi-Detached Binary: In a semi-detached binary system, one star fills its Roche lobe (the region where the gravitational pull of one star dominates) while the other star does not. This can lead to mass transfer between the stars.\\n\\nThese categories help astronomers classify and study eclipsing binary systems based on their physical properties and interactions.',\n",
              "   'reference': {'must_mention': ['primary', 'secondary', 'star', 'types']}},\n",
              "  'd066d3ea-7251-4364-927b-ecc3cb47ac50': {'input': {'question': 'What is the primary method used to study eclipsing binary stars?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear and detailed explanation of the primary method used to study eclipsing binary stars, which is the photometric method. It explains how this method works, what it involves, and what kind of information it can provide. \\n\\nThe submission is insightful as it not only mentions the method but also explains how it is used to determine various properties of the stars. This gives a deeper understanding of the topic.\\n\\nThe submission is appropriate as it directly answers the question asked in the input and stays on topic throughout. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3d8c42b9-4378-466c-8979-543b692f012f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies photometry as the primary method used to study eclipsing binary stars. The student also accurately describes the process of using photometry to study these stars, including the analysis of light curves and the determination of various properties of the stars. The student's answer aligns with the context provided, which includes 'photometry', 'light', and 'curves'. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f3a98f97-13a7-4e62-a08b-9e6bac78f566'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d63ab879-465c-4372-846d-962cd59397a1'), target_run_id=None)],\n",
              "   'execution_time': 2.018882,\n",
              "   'run_id': '9a54f1aa-2cce-49f7-ac24-49069e64d138',\n",
              "   'output': 'The primary method used to study eclipsing binary stars is the photometric method. This method involves measuring the changes in brightness of the binary star system as one star passes in front of the other, causing an eclipse. By analyzing the light curve of the system, astronomers can determine various properties of the stars, such as their sizes, masses, temperatures, and orbital parameters. This method provides valuable information about the dynamics and characteristics of eclipsing binary stars.',\n",
              "   'reference': {'must_mention': ['photometry', 'light', 'curves']}},\n",
              "  'a491bea2-2d7a-4787-b5a1-92578125d82d': {'input': {'question': 'Why are eclipsing binaries important for astronomical measurements?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of why eclipsing binaries are important for astronomical measurements. It lists five reasons, each with a clear explanation of how eclipsing binaries contribute to different aspects of astronomical measurements. \\n\\nThe submission is helpful because it provides a comprehensive answer to the question. It is insightful because it delves into the specifics of how eclipsing binaries are used in astronomy, providing information that may not be commonly known. The submission is appropriate because it directly answers the question and stays on topic throughout.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4f0314c2-957c-41ef-8867-9cad0830bd26'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and covers all the points mentioned in the context. The student explains how eclipsing binaries help in determining stellar parameters, measuring distances, testing stellar evolution models, studying stellar variability, and detecting exoplanets. All these points are factually correct and relevant to the question. The student's answer does not contradict any information given in the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('035ceacb-0046-4e76-b697-91c463a20968'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('469989be-562a-400b-b343-34474ee2cc76'), target_run_id=None)],\n",
              "   'execution_time': 6.395127,\n",
              "   'run_id': '6b407838-b694-418e-a51d-a9a1429b8862',\n",
              "   'output': 'Eclipsing binaries are important for astronomical measurements because they provide valuable information about the properties of stars. Here are some reasons why eclipsing binaries are significant:\\n\\n1. **Determination of Stellar Parameters**: Eclipsing binaries consist of two stars that orbit each other in such a way that they periodically eclipse each other as seen from Earth. By studying the light curve of these eclipses, astronomers can determine the sizes, masses, temperatures, and luminosities of the stars in the binary system. This information helps in understanding the fundamental properties of stars.\\n\\n2. **Distance Measurements**: Eclipsing binaries can be used to determine distances to star clusters and galaxies. By measuring the apparent brightness of the stars in the binary system and comparing it to their known intrinsic brightness, astronomers can calculate the distance to the system using the inverse square law of light.\\n\\n3. **Testing Stellar Evolution Models**: Eclipsing binaries provide a unique opportunity to test stellar evolution models. By comparing the observed properties of the stars in a binary system with the predictions of theoretical models, astronomers can improve our understanding of how stars evolve over time.\\n\\n4. **Studying Stellar Variability**: Eclipsing binaries exhibit variations in their light curves due to the eclipses and other phenomena such as star spots and pulsations. Studying these variations can provide insights into the internal structure and dynamics of stars.\\n\\n5. **Detection of Exoplanets**: Eclipsing binaries can also be used to detect exoplanets orbiting around one of the stars in the binary system. The presence of a planet can cause additional variations in the light curve, allowing astronomers to infer the existence of the exoplanet.\\n\\nOverall, eclipsing binaries play a crucial role in advancing our understanding of stellar astrophysics and are valuable tools for making precise astronomical measurements.',\n",
              "   'reference': {'must_mention': ['distance',\n",
              "     'mass',\n",
              "     'diameter',\n",
              "     'luminosity']}},\n",
              "  '58e455dc-4f8f-4062-a678-8de8bc098db5': {'input': {'question': 'What can the light curves of eclipsing binaries tell us?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, it provides a detailed explanation of what the light curves of eclipsing binaries can tell us. It lists five different pieces of information that can be gleaned from these light curves, including the orbital period, stellar radii, stellar masses, stellar luminosities, and temperature and composition. Each point is explained in a way that is easy to understand, and the submission concludes with a summary of why this analysis is important. \\n\\nThe submission is insightful because it not only lists the information that can be obtained, but also explains how this information is derived and why it is important. \\n\\nThe submission is appropriate because it directly answers the question that was asked. It does not include any irrelevant or inappropriate information. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('80b9dbfe-33af-445b-bce1-e8ae99aa35a2'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is more detailed than the context provided, but it does not contradict it. The context mentions 'period', 'brightness', and 'changes', all of which are covered in the student's answer. The student talks about the 'orbital period', which aligns with 'period' from the context. 'Brightness' is covered in the student's points about 'stellar luminosities' and 'temperature and composition'. 'Changes' is a broad term that could apply to any of the points the student made, as they all involve changes in the light curve. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9234ebfa-6f84-4207-b6fc-b627e295f151'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('59802b24-8d99-4d2c-965b-ef0b0f967261'), target_run_id=None)],\n",
              "   'execution_time': 5.168854,\n",
              "   'run_id': '7a5cb85e-23ab-46c2-acc4-ab1cf0561a34',\n",
              "   'output': 'The light curves of eclipsing binaries can tell us several important things about the binary star system:\\n\\n1. **Orbital Period**: By analyzing the periodic variations in the light curve, astronomers can determine the orbital period of the binary system. This is the time it takes for one star to orbit around the other.\\n\\n2. **Stellar Radii**: The shape and duration of the eclipses in the light curve can provide information about the sizes of the stars in the binary system. By measuring the depth and duration of the eclipses, astronomers can calculate the radii of the stars.\\n\\n3. **Stellar Masses**: The masses of the stars in the binary system can be determined by studying the orbital dynamics of the system, as well as the timing and shape of the eclipses in the light curve.\\n\\n4. **Stellar Luminosities**: By combining information about the radii and temperatures of the stars in the binary system, astronomers can calculate the luminosities of the stars. This can provide insights into the evolutionary stage of the stars.\\n\\n5. **Temperature and Composition**: The light curve can also reveal information about the temperature and composition of the stars in the binary system. This can help astronomers understand the physical properties of the stars.\\n\\nOverall, the analysis of light curves of eclipsing binaries is crucial for understanding the properties and dynamics of binary star systems, as well as for testing theoretical models of stellar evolution.',\n",
              "   'reference': {'must_mention': ['period', 'brightness', 'changes']}},\n",
              "  'f1581525-4091-4305-aa91-a387687dfdd9': {'input': {'question': 'How does the inclination of the orbital plane affect the observation of eclipsing binaries?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of how the inclination of the orbital plane affects the observation of eclipsing binaries. It explains what eclipsing binaries are and how the inclination of the orbital plane can result in different types of eclipses (total or partial) and variations in the light curve. \\n\\nThe submission is insightful as it provides a clear understanding of the topic and is appropriate as it directly answers the question asked in the input. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6f91aa9b-7722-4c30-882e-0fbb944b4710'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The student correctly explains that the inclination of the orbital plane of eclipsing binaries affects how we observe them. They correctly state that if the orbital plane is inclined such that the stars pass directly in front of each other as seen from Earth, we will observe total eclipses. Conversely, if the orbital plane is inclined such that the stars do not pass directly in front of each other, we will observe partial eclipses. The student also correctly notes that the depth and duration of the eclipses will vary depending on the inclination of the orbital plane. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f7e9ade4-9f3f-444b-974d-f3bf14af7d55'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('02290314-dd01-43aa-8172-baf4bdb1f170'), target_run_id=None)],\n",
              "   'execution_time': 4.124316,\n",
              "   'run_id': '3fd6c13e-e089-45b3-80b8-9646a2965e02',\n",
              "   'output': 'The inclination of the orbital plane of eclipsing binaries affects the way we observe them. Eclipsing binaries are binary star systems where one star passes in front of the other as seen from Earth, causing periodic changes in brightness. \\n\\nThe inclination of the orbital plane determines the angle at which we view the binary system. If the orbital plane is inclined such that the stars pass directly in front of each other as seen from Earth (i.e., the orbital plane is edge-on), we will observe total eclipses where one star completely blocks the light from the other. \\n\\nOn the other hand, if the orbital plane is inclined such that the stars do not pass directly in front of each other (i.e., the orbital plane is not edge-on), we will observe partial eclipses where only a portion of one star is blocked by the other. The depth and duration of the eclipses will vary depending on the inclination of the orbital plane.\\n\\nIn summary, the inclination of the orbital plane of eclipsing binaries affects the type of eclipses we observe (total or partial) and the characteristics of the light curve (brightness variations) that we can analyze to study the properties of the binary system.',\n",
              "   'reference': {'must_mention': ['inclination',\n",
              "     'orbital',\n",
              "     'visible',\n",
              "     'eclipse']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "#### That function evaluate the helpfulness of responses in a conversation. It first extracts the initial query and the final response from the conversation state. The function then uses a prompt template combined with a model to determine if the final response is extremely helpful (denoted by \"Y\") or not (denoted by \"N\"), and it provides feedback based on this assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWOPweuSiHc"
      },
      "source": [
        "Let's compile and test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "7e2ea696-ba42-468e-a842-2c585f246c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: Related to astronomy, what is an eclipsing binary star? Also, what are the main types of eclipsing binaries? Also, how does the orbital plane's inclination affect the observation of eclipses?\n",
            "\n",
            "\n",
            "Agent Response: An eclipsing binary star is a binary star system in which the two stars orbit around a common center of mass and periodically eclipse each other as seen from Earth. This results in a periodic variation in the system's brightness as one star passes in front of the other, causing an eclipse.\n",
            "\n",
            "There are two main types of eclipsing binaries based on the shape of their light curves:\n",
            "\n",
            "1. **Algol-type Eclipsing Binaries**: In Algol-type binaries, the primary star is more massive than the secondary star. The light curve shows a deep primary eclipse when the less massive secondary star passes in front of the more massive primary star.\n",
            "\n",
            "2. **Beta Lyrae-type Eclipsing Binaries**: In Beta Lyrae-type binaries, both stars are of similar mass. The light curve shows a continuous variation in brightness as the stars pass in front of each other, with no clear distinction between primary and secondary eclipses.\n",
            "\n",
            "The orbital plane's inclination affects the observation of eclipses in the following ways:\n",
            "\n",
            "- If the orbital plane is inclined such that it is nearly edge-on to the observer, the eclipses will be total and the light curve will show deep primary and secondary eclipses.\n",
            "- If the orbital plane is inclined such that it is nearly face-on to the observer, the eclipses will be partial and the light curve will show shallow eclipses.\n",
            "- The inclination angle also affects the duration and shape of the eclipses, as well as the timing of the eclipses relative to the orbital period of the system.\n",
            "\n",
            "Overall, the study of eclipsing binary stars provides valuable information about the properties of the stars in the system, such as their masses, radii, temperatures, and luminosities.\n"
          ]
        }
      ],
      "source": [
        "# inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to astronomy, what is an eclipsing binary star? Also, what are the main types of eclipsing binaries? Also, how does the orbital plane's inclination affect the observation of eclipses?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
